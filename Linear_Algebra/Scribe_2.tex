% This is a borrowed LaTeX template file for lecture notes for CS267,
% Applications of Parallel Computing, UCBerkeley EECS Department.

% To familiarize yourself with this template, the body contains
% some examples of its use.  Look them over.

\documentclass[a4paper]{article}

%
% ADD PACKAGES here:
%

\usepackage{amsmath,amsfonts,graphicx,multicol}
\usepackage[margin=1in]{geometry}
\usepackage{amsthm}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{bm,nicefrac}
\usepackage{hyperref}

\setlength{\parskip}{\baselineskip} % Set space between paras
\setlength{\parindent}{0pt} % No para indentation

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{problem}{Problem}
\newtheorem{statement}{Statement}

\begin{document}

\pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \noindent
   \begin{center}
   \framebox
   {
      \vbox{\vspace{2mm}
        \hbox to 6.28in { {\Large \hfill Linear algebra \hfill} } %% Fill here
        \vspace{2mm}
        \hbox to 6.28in { {\it Chaitanya Kharyal, Ishita Vohra \hfill} } %% Fill here
        \vspace{2mm}}
   }
   \end{center}
   \markboth{}{} %% Fill here

\section{Notation:}
We have tried to use uppercase letters ($\mathbf{A}$) for matrices, lowercase bold letters ($\mathbf{x}$) for vectors and lowercase letters ($x$) for scalars (constants and variables). We have tried to stick to this notation as much as possible but there might be some unintentional slips and we apologise for that.

\section{Topics to be covered}
\begin{itemize}
    \item $\mathbf{A} = \mathbf{L}\mathbf{U}$
    \item Eigen values and Eigen vectors
    \item Singular Value Decomposition (SVD)
\end{itemize}

\section{$\mathbf{A} = \mathbf{L}\mathbf{U}$}
    \subsection{representing row operations as matrices}
        suppose we take the matrix,
        \begin{center}
            $
            \mathbf{A} = 
            \begin{bmatrix}
                a & b & c & d \\ 
                e & f & g & h \\
                i & j & k & l \\
                m & n & o & p 
            \end{bmatrix}
            $
        \end{center}
        
        And, we want to perform the operation $R_2 = R_2 - R_1$. (Note that while converting a matrix to Row Echelon form, we always perform the operations of type $R_i = R_i + cR_j$, where $i > j$). What will be the matrix for this operation?
        
        \subsubsection{Row picture of matrix multiplication}
            Just like the Column picture of matrix multiplication, you can interpret a matrix multiplication as Row picture. Suppose we have a matrix $\mathbf{A}$ and we multiply a row vector $\mathbf{x'}$ to its left. Formally,
            
            \begin{center}
                $
                \mathbf{x'}\mathbf{A} = \mathbf{b'}
                $
                
                $
                \begin{bmatrix}
                x_1   &
                x_2   &
                \dots &
                x_m 
                \end{bmatrix}
                \begin{bmatrix}
                a_{1,1} & a_{1,2} & \dots & a_{1,n} \\ 
                a_{2,1} & a_{2,2} & \dots & a_{2,n} \\
                \vdots  & \vdots  &       & \vdots  \\
                a_{m,1} & a_{m,2} & \dots & a_{m,n} 
                \end{bmatrix}
                 = 
                \begin{bmatrix}
                b_1   &
                b_2   &
                \dots &
                b_n
                \end{bmatrix}
                $
                
                $
                \begin{bmatrix}
                x_1   &
                x_2   &
                \dots &
                x_m 
                \end{bmatrix}
                \begin{bmatrix}
                \rule[.5ex]{2.5em}{0.4pt} & \mathbf{r_1} & \rule[.5ex]{2.5em}{0.4pt} \\ 
                \rule[.5ex]{2.5em}{0.4pt} & \mathbf{r_2} & \rule[.5ex]{2.5em}{0.4pt} \\
                   & \vdots  &  \\
                \rule[.5ex]{2.5em}{0.4pt} & \mathbf{r_m} & \rule[.5ex]{2.5em}{0.4pt}
                \end{bmatrix}
                 = 
                \begin{bmatrix}
                b_1   &
                b_2   &
                \dots &
                b_n
                \end{bmatrix}
                $ \dots\dots \texttbf{(1)}
            \end{center}
            Where,
            \begin{center}
                $
                    \mathbf{r_i} = 
                    \begin{bmatrix}
                    a_{i,1}   &
                    a_{i,2}   &
                    \dots     &
                    a_{i,n} 
                    \end{bmatrix}
                $
            \end{center}
            
            Then, equation \textbf{(1)} can be written as,
            \begin{center}
                $
                \displaystyle\sum_{i=1}^{m}x_i\textbf{r}_i = \textbf{b} \dots\dots \textbf{(2)}
                $
            \end{center}
        
            Verify that \textbf{(2)} is correct.
            
            This can easily be extended to multlipication of two matrices (instead of a matrix and a vector). For example,
            
            \begin{center}
                $
                    \begin{bmatrix}
                x_1   & x_2   & \dots & x_m \\
                y_1   & y_2   & \dots & y_m
                \end{bmatrix}
                \begin{bmatrix}
                \rule[.5ex]{2.5em}{0.4pt} & \mathbf{r_1} & \rule[.5ex]{2.5em}{0.4pt} \\ 
                \rule[.5ex]{2.5em}{0.4pt} & \mathbf{r_2} & \rule[.5ex]{2.5em}{0.4pt} \\
                   & \vdots  &  \\
                \rule[.5ex]{2.5em}{0.4pt} & \mathbf{r_m} & \rule[.5ex]{2.5em}{0.4pt}
                \end{bmatrix}
                 = 
                \begin{bmatrix}
                \displaystyle\sum_{i=1}^{m}x_i\textbf{r}_i \\
                \displaystyle\sum_{i=1}^{m}y_i\textbf{r}_i
                \end{bmatrix}
                $
            \end{center}
            
        \subsubsection{Row operations as matrices}
            Now that we know how to represent matrix multlipication in the row picture, we can continue our discussion on finding the matrix for operation $R_2 = R_2 - R_1$. Using the row picture, we know the vector that represents $R_2 - R_1$ is,
            
            \begin{center}
                $
                \begin{bmatrix}
                    -1 & 1 & 0 & 0 
                \end{bmatrix}
                $
            \end{center}
            
            But, we also want to keep all the other rows as they are. Therefore, the matrix becomes,
            
            \begin{center}
                $
                    \begin{bmatrix}
                    1  & 0 & 0 & 0 \\
                    -1 & 1 & 0 & 0 \\
                    0  & 0 & 1 & 0 \\
                    0  & 0 & 0 & 1 
                    \end{bmatrix}
                $
            \end{center}
            
            You can multiply this matrix to $\mathbf{A}$ and see that this, in fact, performs the operation $R_2 = R_2 - R_1$. Note that this is a \textbf{Lower Triangular matrix}.
            \begin{definition}
                \textbf{Lower Triangular matrix: } A square matrix, $\mathbf{A}$ is called Lower Triangular Matrix iff $[\mathbf{A}]_{(i,j)}$ ($(i,j)^{th}$ entry of $\mathbf{A}$) $ = 0$ if $j>i$. 
            \end{definition}
            
            We take this opportunity to define what is an Upper Triangular matrix.
            \begin{definition}
                \textbf{Upper Triangular matrix: } A square matrix, $\mathbf{A}$ is called Lower Triangular Matrix iff $[\mathbf{A}]_{(i,j)} = 0$ if $i>j$. 
            \end{definition}
            
            \newpage
            
            Suppose, now we want to apply an operation $R_3 = R_3 + R_2 + R_1$, the matrix for that operation will be,
            
            \begin{center}
                $
                    \begin{bmatrix}
                    1  & 0 & 0 & 0 \\
                    0  & 1 & 0 & 0 \\
                    1  & 1 & 1 & 0 \\
                    0  & 0 & 0 & 1 
                    \end{bmatrix}
                $
            \end{center}
            
            And the complete operation ($R_2 = R_2 + R_1$ and $R_3 = R_3 + R_2 + R_1$) becomes,
            
            \begin{center}
                $
                    \begin{bmatrix}
                    1  & 0 & 0 & 0 \\
                    0  & 1 & 0 & 0 \\
                    1  & 1 & 1 & 0 \\
                    0  & 0 & 0 & 1 
                    \end{bmatrix}
                    \begin{bmatrix}
                    1  & 0 & 0 & 0 \\
                    -1 & 1 & 0 & 0 \\
                    0  & 0 & 1 & 0 \\
                    0  & 0 & 0 & 1 
                    \end{bmatrix}
                    \mathbf{A}
                    $
                    
                    $
                    =
                    \begin{bmatrix}
                    1  & 0 & 0 & 0 \\
                    -1 & 1 & 0 & 0 \\
                    0  & 1 & 1 & 0 \\
                    0  & 0 & 0 & 1 
                    \end{bmatrix}
                    \mathbf{A}
                $
            \end{center}
            
            Note that the product is also Lower Triangular matrix. \textbf{[See problem 1]}
            
    \subsection{LU decomposition}
        While converting a matrix to row echelon form, we multiply the row operation matrices to convert it to a upper triangular matrix. Because all the operations while converting to row echelon form are of the form $R_i = R_i + cR_j \text{with} i<j$, the row operation matrix is always Lower Triangular. If we stack all these matrices to form a single Lower Triangular matrix $T$. The equation, Then, becomes,
        
        \begin{equation*}
            \mathbf{TA} = \mathbf{U}
        \end{equation*}
        \begin{equation*}
            \implies \mathbf{A} = \mathbf{T^{-1}U}
            \implies \mathbf{A} = \mathbf{LU}
        \end{equation*}
        Here, $\mathbf{L}$ is also a Lower Triangular matrix. \textbf{[See problem 2]}
    \subsection{Solving Ax = b using LU}
        First let's see how you can solve $\mathbf{Lx = v}$ where $\mathbf{L}$ is lower triangular matrix. 
        \begin{center}
        $
         \begin{bmatrix}
                a_{1,1} & 0 & \dots & 0 \\ 
                a_{2,1} & a_{2,2} & \dots & 0 \\
                \vdots  & \vdots  &       & \vdots  \\
                a_{m,1} & a_{m,2} & \dots & a_{m,n} 
        \end{bmatrix}
        \begin{bmatrix}
                x_1 \\
                x_2 \\
                \dots \\
                x_n
        \end{bmatrix}
                 = 
                \begin{bmatrix}
                b_1   \\
                b_2   \\
                \dots \\
                b_m
                \end{bmatrix}
        $
        \end{center}
        hence we have 
        \begin{equation*}
            x_1 = b_1/a_{1,1} 
        \end{equation*}
        \begin{equation*}
            x_2 = (b_2-a_{2,1}*x_1)/a_{2,2} 
        \end{equation*}
         \begin{equation*}
            x_3 = (b_3-a_{3,1}*x_1-a_{3,2}*x_2)/a_{3,3} 
        \end{equation*}
        \begin{equation*}
            \vdots
        \end{equation*}
         \begin{equation*}
            x_n = (b_3-a_{n,1}*x_1-a_{n,2}*x_2\dots-a_{n,n-1}*x_{n-1})/a_{n,n}
        \end{equation*}
        
    Similarly if we have $\mathbf{Ux = c}$
    where U is upper triangular matrix we can solve the value of $\mathbf{x}$ in a similar fashion. 
    
    Now for solving $\mathbf{Ax = b}$ using LU.
    \begin{itemize}
        \item Convert $\mathbf{A}$ into $\mathbf{LU}$
        \item So now we have $\mathbf{LUx = b}$. Solve $\mathbf{Lv = b}$.
        \item Now solve $\mathbf{Ux = b}$. Hence we get the value of $\mathbf{x}$. 
    \end{itemize}
        
        
        
\section{Eigen values and Eigen vectors}
    Let $\mathbf{T}$ be a Linear Transformation and $\mathbf{v}$ be a vector such that after applying $\mathbf{T}$ to $\mathbf{v}$, $\mathbf{v}$ remains on its span. Formally,
    
    \begin{equation*}
        \mathbf{Tv} \; = \; \lambda \mathbf{v}
    \end{equation*}
    
    Such vector $\mathbf{v}$ is called \textbf{eigen vector} of $\mathbf{T}$ and the corresponding $\lambda$ is called \textbf{eigen value} corresponding to this eigen vector. Eigen vector is defined to be a non-zero vector.
    
    \subsection{Properties}
    \begin{statement}
        $\lambda$ is an eigen value of  $\mathbf{A}$ if and only if $\mathbf{A-(\lambda)I}$ is singular and the eigen vector $\mathbf{v}$ belongs to null space of $\mathbf{A-(\lambda)I}$.
    \end{statement}
    
    \textbf{Explanation:}
    
        Suppose, $\lambda$ is an eigen value for $\mathbf{A}$ and the corresponding eigen vector be $\mathbf{v}$. We know that,
        
        \begin{equation*}
            \mathbf{Av} = \lambda\mathbf{v}
        \end{equation*}
        \begin{equation*}
            \implies \mathbf{Av} = \lambda\mathbf{I}\mathbf{v}
        \end{equation*}
        
        Where, $\mathbf{I}$ is an \textbf{Identity matrix}.
        
        \begin{equation*}
            \implies \mathbf{Av}-\lambda\mathbf{I}\mathbf{v} = 0
        \end{equation*}
        \begin{equation*}
            \implies(\mathbf{A} - \lambda\mathbf{I})\mathbf{v} = 0
        \end{equation*}
        
        Therefore, $\mathbf{A}-\lambda\mathbf{I}$ is a singular matrix since $\mathbf{v}$ is not a zero vector. And $\mathbf{v}$ lies in the null space of $\mathbf{A}-\lambda\mathbf{I}$.  
        
        Also, for finding the eigen values, we solve the equation $\text{det}(\mathbf{A}-\lambda\mathbf{I}) = 0$. The polynomial $\text{det}(\mathbf{A}-\lambda\mathbf{I}) = 0$ is called the \textbf{characteristic polynomial} of $\mathbf{A}$.
            
        \begin{statement}
            If $\lambda$ is an eigen value of  $\mathbf{A}$ the $\lambda^{2}$ is the eigen value of $\mathbf{A}^{2}$. The eigen vectors remain the same.
        \end{statement}
        
        \textbf{Explanation:}
        
        We know that,
        
        \begin{equation*}
            \mathbf{Av} = \lambda\mathbf{v}
        \end{equation*}
        We left multiply $\mathbf{A}$ on both sides,
        \begin{equation*}
            \implies \mathbf{AAv} = \lambda\mathbf{Av}
        \end{equation*}
        \begin{equation*}
            \implies \mathbf{A^2v} = \lambda(\lambda\mathbf{v})
        \end{equation*}
        \begin{equation*}
            \implies \mathbf{A^2v} = \lambda^2\mathbf{v}
        \end{equation*}
    This can be extended to $m^{th}$ power of $\mathbf{A}$
    
    \begin{statement}
        If $\mathbf{A}$ is singular, then $\lambda$ = 0 will be the eigen value of $\mathbf{A}$.
    \end{statement}
        
    \textbf{Explanation:}
    
    If $\mathbf{A}$ is a singular matrix, the columns of $\mathbf{A}$ are not linearly independent. This implies that there is some vector $\mathbf{v}$ such that,
    
    \begin{equation*}
        \mathbf{Av} = \mathbf{0}
    \end{equation*}
    \begin{equation*}
        \implies \mathbf{Av} = 0\mathbf{v}
    \end{equation*}
    
    Therefore, $0$ eigen value. Such eigen vector $\mathbf{v}$ will be a vector in the null space of $\mathbf{A}$. The number of times $0$ will occur as Eigen value will be equal to the \textit{Nullity} of $\mathbf{A}$.
    
    \begin{statement}
        Sum of eigen values gives us the trace of Matrix where product of eigen values gives us the determinant.
    \end{statement}
    
    \textbf{Explanation:}
    
    \begin{equation}
    \det(A-\lambda I_n) \\ 
    =\begin{vmatrix} 
    a_{1 1}- \lambda & a_{1 2} & \cdots & a_{1,n} \\ 
    a_{2 1} & a_{2 2} -\lambda & \cdots & a_{2,n} \\ 
    \vdots & \vdots & \ddots & \vdots \\ 
    a_{n 1} & a_{m 2} & \cdots & a_{n n}-\lambda 
    \end{vmatrix} =\prod_{i=1}^n (\lambda_i-\lambda).
    \end{equation}
    
    Note that, the above equation is nothing but the characteristic polynomial for a matrix $\mathbf{A}$.
    
    On putting $\lambda = 0$, we can see that $det(\mathbf{A}) = \prod_{i=1}^n{\lambda_i}$. Therefore, determinant of $\mathbf{A}$ is the product of eigen values.
    
    Also, compare the co-efficient of $\lambda^{n-1}$ on both sides. On left side, it is $\sum_{i=1}^n{a_{ii}}$, whereas on RHS, it is $\sum_{i=1}^n{\lambda_i}$. For both sides to be equal,
    
    \begin{equation*}
        \sum_{i=1}^n{a_{ii}} = \sum_{i=1}^n{\lambda_i}
    \end{equation*}
    \begin{equation*}
        Trace(\mathbf{A}) = \sum_{i=1}^n{\lambda_i}
    \end{equation*}
    
    \begin{statement}
        Triangluar matrices have the eigen values on their diagonals.
    \end{statement}
    
      The listed properties are equivalent to a drop in ocean of properties of and around Eigen values and vectors. We have tried to list some important basic properties which are used a lot in Linear Algebra.
    
\section{Singular Value Decomposition(SVD)}
\href{https://brilliant.org/wiki/singular-value-decomposition/}{SVD Wiki}

\section{Problems}

\begin{problem}
    Show that product of Upper Triangular matrices is Upper Triangular (same for Lower Triangular matrices).
\end{problem}

\begin{problem}
    Show that inverse of a Lower Triangular matrix is also Lower Triangular (similar for Upper Triangular matrices).
\end{problem}
\end{document}
