% This is a borrowed LaTeX template file for lecture notes for CS267,
% Applications of Parallel Computing, UCBerkeley EECS Department.

% To familiarize yourself with this template, the body contains
% some examples of its use.  Look them over.

\documentclass[a4paper]{article}

%
% ADD PACKAGES here:
%

\usepackage{amsmath,amsfonts,graphicx,multicol}
\usepackage[margin=1in]{geometry}
\usepackage{amsthm}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{bm,nicefrac}
\usepackage{hyperref}

\setlength{\parskip}{\baselineskip} % Set space between paras
\setlength{\parindent}{0pt} % No para indentation

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}

\begin{document}

\pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \noindent
   \begin{center}
   \framebox
   {
      \vbox{\vspace{2mm}
        \hbox to 6.28in { {\Large \hfill Learning Theory \hfill} } %% Fill here
        \vspace{2mm}
        \hbox to 6.28in { {\it Prepared by: Chaitanya Kharyal\hfill} } %% Fill here
        \vspace{2mm}}
   }
   \end{center}
   \markboth{}{} %% Fill here

\section{A Gentle Start}

\subsection{Posing the problem:}
Suppose with no prior experience about papayas, we arrive at some small Pacific Island, where papayas are a significant ingredient in local diet. Now, we have to learn how to predict whether a papaya is tasty or not based on some of its features. We choose these features to be colour and softness. Now, given a set of papayas, how will we determine whether a papaya is tasty or not.

\subsection{A Formal Model - The Statistical Learning Framework}
We define the following terms:
\begin{itemize}
    \item \textbf{Domain Set:} We can call it the universal set, $\chi$,of the objects that we wish to label. For example in papaya labelling example, it is the set of all papayas.
    \item \textbf{Label set: } It is the set, $Y$, of all possible labels. For example in our papaya labelling case, \{\textit{tasty, non-tasty}\} or if we map it to integers, (\textit{1} for \trextit{tasty} and \textit{0} for \trextit{non-tasty}) \{\textit{0, 1}\}.
    \item \textbf{Training Data: } $S = ((x_1, y_1),...,(x_m,y_m))$ is a finite set in $\chi \times Y$. It is the input that learner has access to. For example, in the papaya example, the previously tasted papayas should make the training set for the learner.
    \item \textbf{The learner's output: } What we want from our learner is a prediction rule, which takes as input a papaya (or features of papaya) and, without tasting it, tries to predict whether the papaya is tasty or non-tasty. This prediction rule is called \textit{hypothesis}. We use $A(S)$ to denote a prediction rule which learning algorithm $A$ returns on training set $S$.
    \item \textbf{The data generation model: } (The learner doesn't know anything about this model) First we assume that the set $\chi$ has some distribution related to it. We denote this prob. dist. over $\chi$ by $\mathcal{D}$. We also assume, for now, that there is some correct labelling function, $f: \chi \to Y$, and $y_i = f(x_i)$ for all $i$. note that the aim of the learner is just to approximate this function $f$ given $S$.
    \item \textbf{Measures of success: } 
        \begin{itemize}
            \item \textbf{Generalisation error: } It is defined as the probability of our hypothesis $h$ giving wrong output on a sample chosen randomly with underlying distribution $\mathcal{D}$.
                \begin{center}
                                    $L_{\mathcal{D},f}(h) = \underset{x\sim\mathcal{D}}{\mathbb{P}}]h(x) \neq f(x)] \equiv \mathcal{D}(\{x:h(x) \neq f(x)\})$
                \end{center}
            What does this mean? Look at the middle term in the above equation. It is the probability of finding an $x$ distributed as $\mathcal{D}$ such that $h(x) \neq f(x)$.
                
            Since \mathcal{D} and $f$ are not available to the learner, the generalisation error can't be calculated. Therefore, we try to find the \textit{Empirical error}.
            \item \textbf{Empirical error: } It is the mean error of our hypothesis on the sample set $S$.
                \begin{center}
                    $L_S(h) \equiv \frac{|\{i\in[m]:h(x_i) \neq y_i\}|}{m}$
                \end{center}
            where $|S| = m$.
            
            Therefore, it can also be defined as the mean error on our sample.
            
        \end{itemize}
        \end{itemize}
        \newpage
        
    \subsection{Empirical Risk Minimisation}
    Since our learner doesn't have any information about $\mathcal{D}$ and the labelling function $f$, it is not possible to minimize the generalisation error unless we are given all the infinite samples (Note that knowing all the samples and the labels of each of them will be equivalent to know everything about the data moreover, if we have all the data with labels, there is no need for learning anything at all). Therefore, it makes sense to minimise the empirical error, which depends only on the training set.
        \subsubsection{Why does it make sense to minimise empirical risk?}
        Since the empirical risk depends only on the sample set $S$, the intuitive question to ask here is "Why does the empirical risk represent anything about the real data $\chi$?". Why should we minimise empirical risk even if we don't have true error?
        
        \textbf{Statement: } $E[L_S(h)] = L_{\mathcal{D},f}(h)$ \\
        \textbf{Explanation: } \\
            Let,
            \begin{center}
              
            \[ one(h,f,x) =
                  \begin{cases}
                    1   & \quad \text{if }\text{ $h(x) \neq f(x)$ }\\
                    0  & \quad \text{ otherwise}
                  \end{cases}
                \]
            \end{center}
            
            So, we know that,
            \begin{center}
                $L_S(h) = \frac{1}{m}\displaystyle\sum_{i=1}^{m}one(h,f,x_i)$
            \end{center}
            Therefore,
            
            \begin{center}
                $E(L_S(h) = \underset{S|x\sim\mathcal{D}^m}{E}[\frac{1}{m}\displaystyle\sum_{i=1}^{m}one(h,f,x_i)]$
                
                \hspace{12px} $=\frac{1}{m}\displaystyle\sum_{i=1}^{m}E[one(h,f,x_i)]$
                
                \hspace{35px}$=(\frac{1}{m} \times m) \underset{x \sim \mathcal{D}}{P}[h(x) \neq f(x)]$
                
                \hspace{-45px} $=L_{\mathcal{D},f}(h)$
            \end{center}
        
    \subsubsection{Overfitting (ERM might fail!)}
        Sometimes ERM might output something that fits the sample data $S$ a bit 'too well'. For example, in out papaya example, ERM can output a function $h_S(x)$, such that,
        \begin{center}
            \[ h_S(x) =
                  \begin{cases}
                    y_i   & \quad \text{if }\text{ $x = x_i : x_i \in S$ }\\
                    0  & \quad \text{ otherwise}
                  \end{cases}
                \]
        \end{center}
        
        In such case, even though the empirical loss $L_S(h)$ is $0$ (because it perfectly predicts the tastiness for every papaya in $S$), we know that it is not the true function unless $S$ contains all the tasty papayas. This means we have found a function which performs perfectly on the training set but fails miserably in the real world. Such problem is called \textit{overfitting}. To tackle this problem, we introduce something called \textit{ERM with inductive bias}.
    
    \subsection{ERM with Inductive Bias}
        Since, normal ERM has the problem of overfitting, we try to fix it by introducing something called as \textit{inductive bias}. To tackle the problem of overfitting, we make our learner to search in a restricted search space. We call the set of this space as the \textit{hypothesis space}, $\mathcal{H}$. Ideally the learner should choose the hypothesis class before looking at the training set.
        
        The ERM with inductive bias, for a given class $\mathcal{H}$ and a given training set $S$, returns a function $h_S \in \mathcal{H}$ such that,
        \begin{center}
            $ERM_h(S) \in \underset{h \in \mathcal{H}}{argmin}$  \vspace{5px} $L_S(h)$
        \end{center}
        
        \subsubsection{Finite hypothesis class}
            in ERM with inductive bias, we have restricted our search space to $\mathcal{H}$. Now, we impose a simple restriction on $\mathcal{H}$, ie. we upper-bound the size of $\mathcal{H}$ or assume it to be finite.
            
            Now, to analyse the performance of ERM{$_\mathcal{H}$}, we need some definitions and assumptions.
            
            \begin{definition}
                \textbf{The realizability assumption: } There exists $h^* \in \mathcal{H}$ such that $L_{(\mathcal(D),f)} = 0$.
            \end{definition}
            
            Note that, Realizability assumption implies that for any random $S$ sampled according to $\mathcal{D}$ and labelled according to $f$, we have $L_S(h^*) = 0$.
            
            \begin{definition}
                \textbf{The i.i.d. assumption: } The examples in the training set are independently and identically distributed according to the distribution $\mathcal{D}$. 
            \end{definition}
            
            Intuitively, the larger the sample set gets, the more likely is it to represent the distribution and the labelling function.
            
            Since, we pick $S$ randomly, there is an intrinsic randomness in the output of our algorithm $h_S$ and thus, $L_{(\mathcal{D},f)}(h_S)$. Therefore, it is unrealistic to expect that with full certainty, our algorithm will output the correct hypothesis. Thus, we use the bounds over the probability to measure our success. Sometimes, our sample set might be non-representative of the original distribution $\mathcal{D}$.
            Usually, we denote the probability of getting such non-representative sample by $\delta$. Also, since we can't guarantee perfect prediction, we introduce another parameter for the quality of prediction, called the accuracy parameter, denoted by $\epsilon$.
            
            \subsection{Bounding the number of samples required}
            The event $L_{(\mathcal{D},f)}(h_S) > \epsilon$ is interpreted as a failure of the learner. We want to upper bound the number of samples required such that failure occurs or we can say that we want to lower bound the number of samples such which are required to guarantee \textit{Probably Approximately Correct (PAC) learning}. Formally, let $S|_x = {x_1,\dots,x_m}$ be an instance of the training set, we would like to upper bound the probability of getting such set, ie,
            \begin{center}
                $
                \mathcal{D}^{m}(\{S|_x : L_{(\mathcal{D},f)}(h_S) > \epsilon\})
                $
            \end{center}
            
            Now, let $\mathcal{H}_B$ be the set of bad hypothesis, ie,
            \begin{center}
                $\mathcal{H}_B = \{h \in \mathcal{H}: L_{(\mathcal{D},f)}(h_S) > \epsilon\}$
            \end{center}
            And, let $M$ be the set of misleading samples ie.
            \begin{center}
                $
                M = \{S|_x:\exists h \in \mathcal{H}_B, L_S(h) = 0\}
                $
            \end{center}
            
            Note that the set of misleading samples is the problem because of such samples, there exists a bad hypothesis which looks good on the sample set.
            Now, since $\underset{\mathcal{H}}{L_S(h)} = 0$ because of realizability assumption, our algorithm can fail (output gives $\L_{(\mathcal{D},f)}(h_S) > \epsilon$) if we get a misleading sample. Formally $M$ can be written as,
            \begin{center}
                $
                    M = \underset{h\in\mathcal{H}_B}{\bigcup}\{S|_x:L_S(h) = 0\}
                $
                
                $
                    \implies \mathcal{D}^m(\{S|_x: L_{(\mathcal{D},f)}(h_S) >\epsilon\})\; \leq \;\mathcal{D}^m(M)\; =\; \mathcal{D}^m(\underset{h\in\mathcal{H}_B}{\bigcup}\{S|_x:L_S(h) = 0\})
                $
                
                $
                    \leq \displaystyle\sum_{h\in \mathcal{H}_B}\mathcal{D}^m(\{S|_x:L_S(h) = 0\})
                $ \dots\dots (Union bound) \dots\dots \textbf{(1)}
            \end{center}
            
            If we fix some sample set $S$, the event $L_S(h) = 0$ is same as saying that $\forall i, h(x_i) = f(x_i)$. Formally,
            \begin{center}
                $
                    \mathcal{D}^m(\{S|_x:L_S(h) = 0\}) =  \mathcal{D}^m(\{S|_x:\forall i, h(x_i) = f(x_i)\}) 
                    
                    = \displaystyle\prod_{i = 1}^m \mathcal{D}(\{x_i: h(x_i) = f(x_i)\})
                $
            \end{center}
            
            Also, we know that $\; \mathcal{D}(\{x_i: h(x_i) = f(x_i)\}) = 1-L_{(\mathcal{D},f)}(h) \; \leq \; 1-\epsilon$
            
            \begin{center}
                $
                    \mathcal{D}^m(\{S|_x:L_S(h) = 0\}) \; \leq \; (1-\epsilon)^m \; \leq \; e^{-\epsilon m}
                $
            \end{center}
            
            Therefore, \textbf{(1)} becomes,
            
            \begin{center}
                $
                    \mathcal{D}^m(\{S|_x: L_{(\mathcal{D},f)}(h_S) >\epsilon\}) \: \leq \: |\mathcal{H}_B|e^{-\epsilon m} \; \leq \; |\mathcal{H}|e^{-\epsilon m}
                    
                    \implies m \; \geq \; \frac{log(|\mathcal{H}|/\delta}{\epsilon} 
                $
            \end{center}
            
            Therefore, to avoid failure (probabilistically), $m$ should satisfy the above equation.
            
\end{document}
